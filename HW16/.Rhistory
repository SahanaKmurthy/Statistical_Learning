df         <- read.csv("http://www.stat.cmu.edu/~pfreeman/breastcancer.csv",stringsAsFactors=TRUE)
response   <- df[,1]  # B for benign, M for malignant
predictors <- data.frame(scale(df[,-1]))
df         <- cbind(predictors,"Label"=response)
cat("Sample size: ",length(response),"\n")
?glm()
# REPLACE ME WITH CODE
set.seed(100)
s <- sample(length(df$Label) , round(length(df$Label) * 0.7))
df.train <- df[s,]
df.test <- df[-s,]
out.log <- glm(Label ~ ., data = df.train, family = binomial)
predicted_probs <- predict(out.log, newdata = df.test, type = "response")
predicted_labels <- ifelse(predicted_probs >= 0.5 , 1 , 0)
table(df.test$label, predicted_labels)
# REPLACE ME WITH CODE
set.seed(100)
s <- sample(length(df$Label) , round(length(df$Label) * 0.7))
df.train <- df[s,]
df.test <- df[-s,]
out.log <- glm(Label ~ ., data = df.train, family = binomial)
predicted_probs <- predict(out.log, newdata = df.test, type = "response")
predicted_labels <- ifelse(predicted_probs >= 0.5 , 1 , 0)
table(df.test$Label, predicted_labels)
# REPLACE ME WITH CODE
library(FNN)
install.packages("FNN")
# REPLACE ME WITH CODE
library(FNN)
k.max <- 20
mcr.k <- numeric(NA,k.max)
# REPLACE ME WITH CODE
# Load necessary library
library(class)
# Set parameters
k.max <- 20
mcr.k <- numeric(k.max)
# Loop over values of k to calculate MCR
for (kk in 1:k.max) {
knn.out <- knn(train = df.train[, 1:20], test = df.test[, 1:20], cl = df.train[, 21], k = kk)
# Calculate misclassification rate (MCR) for each k
mcr.k[kk] <- mean(knn.out != df.test[, 21])
}
# Identify the optimal number of neighbors
k.min <- which.min(mcr.k)
cat("The optimal number of nearest neighbors is", k.min, "\n")
# REPLACE ME WITH CODE
# Load necessary library
suppressMessages(library(class))
# Set parameters
k.max <- 20
mcr.k <- numeric(k.max)
# Loop over values of k to calculate MCR
for (kk in 1:k.max) {
knn.out <- knn(train = df.train[, 1:20], test = df.test[, 1:20], cl = df.train[, 21], k = kk)
# Calculate misclassification rate (MCR) for each k
mcr.k[kk] <- mean(knn.out != df.test[, 21])
}
# Identify the optimal number of neighbors
k.min <- which.min(mcr.k)
cat("The optimal number of nearest neighbors is", k.min, "\n")
# REPLACE ME WITH CODE
# Load the necessary library
library(class)
k <- 3
# Run KNN with prob=TRUE to get probabilities
knn.out <- knn(train = df.train[, 1:20], test = df.test[, 1:20], cl = df.train[, 21], k = k, prob = TRUE)
# Extract Class 1 probabilities
knn.prob <- attributes(knn.out)$prob
class0_name <- "0"  # Replace with actual name of Class 0 in your dataset if different
# Adjust probabilities so they represent the probability of Class 1
w <- which(knn.out == class0_name)
knn.prob[w] <- 1 - knn.prob[w]  # Flip probabilities for Class 1
# Plot histogram to visualize the Class 1 probabilities
hist(knn.prob, breaks = 20, main = "Histogram of Class 1 Probabilities",
xlab = "Class 1 Probability", col = "lightblue", border = "black")
install.packages("e1071")
# REPLACE ME WITH CODE
suppressMessages(library(e1071))
# REPLACE ME WITH CODE
suppressMessages(library(e1071))
set.seed(100)
# Define a sequence of potential cost values for tuning
cost_values <- 2^seq(-5, 5, by = 1)  # Adjust range if the optimum is near the sequence endpoints
# Tune the SVM with a linear kernel and cross-validation to find the optimal cost parameter
svm_tune <- tune(svm, Label ~ ., data = df.train, kernel = "linear",
ranges = list(cost = cost_values),
tunecontrol = tune.control(cross = 10))
# Extract the best model and the optimal cost parameter
best_model <- svm_tune$best.model
best_cost <- svm_tune$best.parameters$cost
cat("The optimal cost (C) value is:", best_cost, "\n")
# Make predictions on the test set using the best model
test_predictions <- predict(best_model, newdata = df.test[, 1:20])
# Calculate the misclassification rate (MCR) on the test set
mcr <- mean(test_predictions != df.test[, 21])
cat("Test-set Misclassification Rate (MCR):", mcr, "\n")
# Generate the confusion matrix
confusion_matrix <- table(Predicted = test_predictions, Actual = df.test[, 21])
print("Confusion Matrix:")
print(confusion_matrix)
# REPLACE ME WITH CODE
set.seed(100)
# Define sequences of values for cost and degree parameters to tune
cost_values <- 2^seq(-5, 5, by = 1)  # Range of cost values
degree_values <- 2:4  # Polynomial degrees to try
# Tune the SVM with a polynomial kernel, adjusting both cost and degree
svm_tune_poly <- tune(svm, Label ~ ., data = df.train, kernel = "polynomial",
ranges = list(cost = cost_values, degree = degree_values),
tunecontrol = tune.control(cross = 10))
# Extract the best model and the optimal parameters
best_model_poly <- svm_tune_poly$best.model
best_cost_poly <- svm_tune_poly$best.parameters$cost
best_degree_poly <- svm_tune_poly$best.parameters$degree
cat("The optimal cost (C) value is:", best_cost_poly, "\n")
cat("The optimal polynomial degree is:", best_degree_poly, "\n")
# Make predictions on the test set using the best polynomial model
test_predictions_poly <- predict(best_model_poly, newdata = df.test[, 1:20])
# Calculate the misclassification rate (MCR) on the test set
mcr_poly <- mean(test_predictions_poly != df.test[, 21])
cat("Test-set Misclassification Rate (MCR) with polynomial kernel:", mcr_poly, "\n")
# Generate the confusion matrix
confusion_matrix_poly <- table(Predicted = test_predictions_poly, Actual = df.test[, 21])
print("Confusion Matrix with Polynomial Kernel:")
print(confusion_matrix_poly)
# REPLACE ME WITH CODE
set.seed(100)
# Define sequences of values for cost and gamma parameters to tune
cost_values <- 2^seq(-5, 5, by = 1)   # Exponentially spaced values for cost
gamma_values <- 10^seq(-8, -1, by = 1) # Logarithmic sequence for gamma
# Tune the SVM with a radial kernel, adjusting both cost and gamma
svm_tune_radial <- tune(svm, Label ~ ., data = df.train, kernel = "radial",
ranges = list(cost = cost_values, gamma = gamma_values),
tunecontrol = tune.control(cross = 10))
# Extract the best model and the optimal parameters
best_model_radial <- svm_tune_radial$best.model
best_cost_radial <- svm_tune_radial$best.parameters$cost
best_gamma_radial <- svm_tune_radial$best.parameters$gamma
cat("The optimal cost (C) value is:", best_cost_radial, "\n")
cat("The optimal gamma value is:", best_gamma_radial, "\n")
# Make predictions on the test set using the best radial model
test_predictions_radial <- predict(best_model_radial, newdata = df.test[, 1:20])
# Calculate the misclassification rate (MCR) on the test set
mcr_radial <- mean(test_predictions_radial != df.test[, 21])
cat("Test-set Misclassification Rate (MCR) with radial kernel:", mcr_radial, "\n")
# Generate the confusion matrix
confusion_matrix_radial <- table(Predicted = test_predictions_radial, Actual = df.test[, 21])
print("Confusion Matrix with Radial Kernel:")
print(confusion_matrix_radial)
# REPLACE ME WITH CODE
set.seed(202)
# Define sequences of values for cost and gamma parameters to tune, based on slide contents
cost_values <- 10^seq(-1, 1, by = 0.5)     # Cost values from 10^-1 to 10^1
gamma_values <- 10^seq(-1, 1, by = 0.4)    # Gamma values from 10^-1 to 10^1
# Tune the SVM with a radial kernel, adjusting both cost and gamma, and enabling probability estimation
tune.out <- tune(svm, Label ~ ., data = df.train, kernel = "radial",
ranges = list(cost = cost_values, gamma = gamma_values),
probability = TRUE)
# Extract the best model and parameters
best_model_radial <- tune.out$best.model
best_cost <- tune.out$best.parameters$cost
best_gamma <- tune.out$best.parameters$gamma
cat("The estimated optimal values for C and gamma are", best_cost, "and", best_gamma, "\n")
# Make predictions on the test set using the best model with probability estimation enabled
resp.pred <- predict(best_model_radial, newdata = df.test[, 1:20], probability = TRUE)
# Extract Class 1 probabilities
probabilities <- attr(resp.pred, "probabilities")[, "1"]  # Extract Class 1 probability column
# REPLACE ME WITH CODE
set.seed(202)
# Define sequences of values for cost and gamma parameters to tune
cost_values <- 10^seq(-1, 1, by = 0.5)
gamma_values <- 10^seq(-1, 1, by = 0.4)
# Tune the SVM with a radial kernel, adjusting both cost and gamma, enabling probability estimation
tune.out <- tune(svm, Label ~ ., data = df.train, kernel = "radial",
ranges = list(cost = cost_values, gamma = gamma_values),
probability = TRUE)
# Extract the best model and parameters
best_model_radial <- tune.out$best.model
best_cost <- tune.out$best.parameters$cost
best_gamma <- tune.out$best.parameters$gamma
cat("The estimated optimal values for C and gamma are", best_cost, "and", best_gamma, "\n")
# Make predictions on the test set using the best radial model with probability estimation
resp.pred <- predict(best_model_radial, newdata = df.test[, 1:20], probability = TRUE)
# Check the structure of the probabilities attribute
str(attr(resp.pred, "probabilities"))  # To inspect the structure of probabilities
# Extract Class 1 probabilities (replace "1" with the actual label if different)
probabilities <- attr(resp.pred, "probabilities")[, "1"]
# REPLACE ME WITH CODE
set.seed(202)
# Define sequences of values for cost and gamma parameters to tune
cost_values <- 10^seq(-1, 1, by = 0.5)
gamma_values <- 10^seq(-1, 1, by = 0.4)
# Tune the SVM with a radial kernel, adjusting both cost and gamma, enabling probability estimation
tune.out <- tune(svm, Label ~ ., data = df.train, kernel = "radial",
ranges = list(cost = cost_values, gamma = gamma_values),
probability = TRUE)
# Extract the best model and parameters
best_model_radial <- tune.out$best.model
best_cost <- tune.out$best.parameters$cost
best_gamma <- tune.out$best.parameters$gamma
cat("The estimated optimal values for C and gamma are", best_cost, "and", best_gamma, "\n")
# Make predictions on the test set using the best radial model with probability estimation
resp.pred <- predict(best_model_radial, newdata = df.test[, 1:20], probability = TRUE)
# Check the structure of the probabilities attribute
str(attr(resp.pred, "probabilities"))  # To inspect the structure of probabilities
# Extract Class 1 probabilities, using the class label "M" instead of "1"
probabilities <- attr(resp.pred, "probabilities")[, "M"]
# Plot a histogram of the Class 1 probabilities
hist(probabilities, breaks = 20, main = "Histogram of Class 1 Probabilities",
xlab = "Probability", col = "lightblue", border = "black")
# REPLACE ME WITH CODE
# Load the necessary library
library(class)
k <- 3
# Run KNN with prob=TRUE to get probabilities
knn.out <- knn(train = df.train[, 1:20], test = df.test[, 1:20], cl = df.train[, 21], k = k, prob = TRUE)
# Extract Class 1 probabilities
knn.prob <- attributes(knn.out)$prob
class0_name <- "B"
# Adjust probabilities so they represent the probability of Class 1
w <- which(knn.out == class0_name)
knn.prob[w] <- 1 - knn.prob[w]  # Flip probabilities for Class 1
# Plot histogram to visualize the Class 1 probabilities
hist(knn.prob, breaks = 20, main = "Histogram of Class 1 Probabilities",
xlab = "Class 1 Probability", col = "lightblue", border = "black")
