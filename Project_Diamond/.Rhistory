knitr::opts_chunk$set(echo = TRUE)
# Predictions from the best model
predictions_best <- predict(best_model, newdata = test_matrix)
knitr::opts_chunk$set(echo = TRUE)
suppressMessages(library(tidyverse))
suppressMessages(library(caret))
suppressMessages(library(randomForest))
suppressMessages(library(pROC))
diamonds <- read.csv("diamonds.csv")
cat("Structure of the diamonds dataset:\n")
str(diamonds)
cat("\nFirst few rows of the dataset:\n")
head(diamonds)
#converting the categorical data into factors
diamonds$cut <- factor(diamonds$cut, levels = c("Fair", "Good", "Very Good", "Premium", "Ideal"))
diamonds$color <- factor(diamonds$color, levels = c("J", "I", "H", "G", "F", "E", "D"))
diamonds$clarity <- factor(diamonds$clarity, levels = c("I1", "SI1", "SI2", "VS1", "VS2", "VVS1", "VVS2", "IF"))
cat("Number of missing values per column:\n")
print(colSums(is.na(diamonds)))
set.seed(123)
trainIndex <- createDataPartition(diamonds$price, p = 0.8, list = FALSE)
trainData <- diamonds[trainIndex, ]
testData <- diamonds[-trainIndex, ]
cat("Training set size: ", nrow(trainData), "\n")
cat("Test set size: ", nrow(testData), "\n")
model = CatBoostRegressor(
iterations=100,          # Number of trees
learning_rate=0.1,        # Step size shrinkage
depth=6,                  # Depth of trees
loss_function='RMSE',     # Loss function for regression
verbose=100               # Print training progress every 100 iterations
)
install.packages("cat")
pip install catboost
install.packages("catboost")
suppressMessages(library(tidyverse))
suppressMessages(library(caret))
suppressMessages(library(randomForest))
suppressMessages(library(pROC))
suppressMessages(library(catboost))
install.packages("catboost")
suppressMessages(library(tidyverse))
suppressMessages(library(caret))
suppressMessages(library(randomForest))
suppressMessages(library(pROC))
suppressMessages(library(catboost))
suppressMessages(library(tidyverse))
suppressMessages(library(caret))
suppressMessages(library(randomForest))
suppressMessages(library(pROC))
suppressMessages(library(ipred))
bagging_model <- bagging(
price ~ carat + cut + color + clarity + depth + table + x + y + z,
data = trainData,
nbagg = 50  # Number of bootstrapped models
)
# Predict on the test set
bagging_predictions <- predict(bagging_model, testData)
# Calculate RMSE
bagging_rmse <- sqrt(mean((bagging_predictions - testData$price)^2))
cat("RMSE for Bagging Model: ", bagging_rmse, "\n")
# Plot Predicted vs Actual
plot(testData$price, bagging_predictions,
main = "Predicted vs Actual (Bagging)",
xlab = "Actual Price", ylab = "Predicted Price", col = "blue", pch = 16)
abline(0, 1, col = "red")
