lm.pred <- predict(lm.out,newdata=df.pc[-s,])
mean((lm.pred-df.pc$y[-s])^2)
knitr::opts_chunk$set(echo = TRUE)
df <- read.csv("stellar_temperature.csv")
summary(df)
nrow(df)
library(ggplot2)
ggplot(data = df , mapping = aes(y = df$teff)) +
geom_boxplot(outliers = TRUE , outlier.color = "darkred" , fill = "seagreen") +
labs(title = "Boxplot of Effective Temperature (teff)" ,
y = "Effective Temperature")
library(reshape2)
library(dplyr)
# Melt the dataframe to long format for ggplot
df_pred <- df %>% select(-teff)
df_long <- melt(df_pred , id.vars = NULL, measure.vars = names(df_pred))
# Plot multiple boxplots
ggplot(df_long, aes(x = variable, y = value)) +
geom_boxplot(fill = "black" , outlier.colour = "red", outlier.shape = 16, outlier.size = 2) +
labs(title = "Boxplots of All Variables", x = "Variables", y = "Values") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
theme_minimal()
outliers_boxplot <- boxplot.stats(df$teff)$out
outliers_boxplot
df$parallax= sqrt(df$parallax)
ggplot(df, aes_string(x = df$parallax)) +  # aes_string() allows dynamic variable names
geom_histogram(fill = "seagreen", color = "white", bins = 30) +
ggtitle(paste("Histogram of Parallax")) +
xlab("parallax") +
theme_minimal()
df$br_col = sqrt(df$br_col)
ggplot(df, aes_string(x = df$br_col)) +  # aes_string() allows dynamic variable names
geom_histogram(fill = "seagreen", color = "white", bins = 30) +
ggtitle(paste("Histogram of Br_col")) +
xlab("Brcol")
theme_minimal()
df$parallax= sqrt(df$parallax)
ggplot(df, aes_string(x = df$parallax)) +  # aes_string() allows dynamic variable names
geom_histogram(fill = "seagreen", color = "white", bins = 30) +
ggtitle(paste("Histogram of Parallax")) +
xlab("parallax") +
theme_minimal()
df$br_col = log(df$br_col)
ggplot(df, aes_string(x = df$br_col)) +  # aes_string() allows dynamic variable names
geom_histogram(fill = "seagreen", color = "white", bins = 30) +
ggtitle(paste("Histogram of Br_col")) +
xlab("Brcol")
theme_minimal()
cor_matrix <- cor(df[, sapply(df, is.numeric)])
library(corrplot)
corrplot(cor_matrix, method = "circle")
knitr::opts_chunk$set(echo = TRUE)
df <- read.csv("stellar_temperature.csv")
summary(df)
nrow(df)
library(ggplot2)
ggplot(data = df , mapping = aes(y = df$teff)) +
geom_boxplot(outliers = TRUE , outlier.color = "darkred" , fill = "seagreen") +
labs(title = "Boxplot of Effective Temperature (teff)" ,
y = "Effective Temperature")
library(reshape2)
library(dplyr)
# Melt the dataframe to long format for ggplot
df_pred <- df %>% select(-teff)
df_long <- melt(df_pred , id.vars = NULL, measure.vars = names(df_pred))
# Plot multiple boxplots
ggplot(df_long, aes(x = variable, y = value)) +
geom_boxplot(fill = "black" , outlier.colour = "red", outlier.shape = 16, outlier.size = 2) +
labs(title = "Boxplots of All Variables", x = "Variables", y = "Values") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
theme_minimal()
library(reshape2)
library(dplyr)
# Melt the dataframe to long format for ggplot
df_pred <- df %>% select(-teff)
df_long <- melt(df_pred , id.vars = NULL, measure.vars = names(df_pred))
# Plot multiple boxplots
ggplot(df_long, aes(x = variable, y = value)) +
geom_boxplot(fill = "black" , outlier.colour = "red", outlier.shape = 16, outlier.size = 2) +
labs(title = "Boxplots of All Variables", x = "Variables", y = "Values") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
theme_minimal()
outliers_boxplot <- boxplot.stats(df$teff)$out
outliers_boxplot
library(ggplot2)
missing_data <- sapply(df, function(x) sum(is.na(x)))
missing_data
sum(is.na(df))
df.new<-df
sapply(df.new, class)
library(ggplot2)
# Function to create a histogram for a specific variable
create_histogram <- function(df, var) {
ggplot(df, aes_string(x = var)) +  # aes_string() allows dynamic variable names
geom_histogram(fill = "brown", color = "white", bins = 30) +
ggtitle(paste("Histogram of", var)) +
theme_minimal()
}
# Get the names of all numeric variables in the dataset
vars <- names(df)
# Loop through all numeric variables and plot histograms
for (var in vars) {
print(create_histogram(df, var))  # Display the histogram
}
df <- na.omit(df)
cor_matrix <- cor(df[, sapply(df, is.numeric)])
library(corrplot)
corrplot(cor_matrix, method = "circle")
set.seed(101)
s <- sample(length(df$teff) , round(0.7*length(df$teff)))
df.train <- df[s,]
df.test <- df[-s,]
lm.out <- lm(teff ~ ., data = df.train)
summary(lm.out)
y.hat = predict(lm.out, newdata = df.test)
# valid_idx <- !is.na(y.hat) & !is.na(df.test$teff)
# mse <- mean((df.test$teff[valid_idx] - y.hat[valid_idx])^2)
mse <- mean((df.test$teff - y.hat)^2)
residuals <- df.test$teff - y.hat
print(mse)
# Residual Plot
ggplot(data = data.frame(y.hat, residuals), aes(x = y.hat, y = residuals)) +
geom_point() +
geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
labs(title = "Residuals vs Predicted Values", x = "Predicted Values", y = "Residuals") +
theme_minimal()
# Histogram of residuals to check normality
ggplot(data = data.frame(residuals), aes(x = residuals)) +
geom_histogram(binwidth = 100, fill = "blue", color = "white") +
labs(title = "Histogram of Residuals", x = "Residuals", y = "Frequency") +
theme_minimal()
rmse <- sqrt(mse)
print(rmse)
df_plot <- data.frame(
Observed = df.test$teff,
Predicted = y.hat
)
axis_limits <- range(c(df_plot$Observed, df_plot$Predicted))
# Plot: Observed vs Predicted with a diagonal line
ggplot(df_plot, aes(x = Observed, y = Predicted)) +
geom_point(alpha = 0.6, color = "blue") +  # Points for each observation
geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed", linewidth = 1) +  # Diagonal line
xlim(axis_limits) + ylim(axis_limits) +  # Same limits for both axes
labs(title = "Observed vs Predicted Values", x = "Observed", y = "Predicted") +
theme_minimal()
suppressMessages(library(car))
reduced_df <- df.train
lm.out <- lm(teff~.,data=reduced_df)
print(vif(lm.out))
suppressMessages(library(car))
reduced_df <- df.train
lm.out <- lm(teff~.,data=reduced_df)
vif(lm.out)
library(dplyr)
library(bestglm)
set.seed(101)
# Ensure 'y' is numeric after renaming
df.renamed <- df %>% rename(y = teff)
df.renamed$y <- as.numeric(df.renamed$y)  # Ensure y is numeric
# Split the data into training and test sets
s <- sample(length(df.renamed$y), round(0.7 * length(df.renamed$y)))
df.train <- df.renamed[s, ]
df.test <- df.renamed[-s, ]
df.train <- na.omit(df.train)
df.test <- na.omit(df.test)
# Fit the full linear model
lm.out <- lm(y ~ ., data = df.train)
# Compute the MSE for the full model
mse.full <- mean((predict(lm.out, newdata = df.test) - df.test$y) ^ 2)  # df.test$y, not df.test$teff
print(paste("MSE of the full model:", mse.full))
# Perform best subset selection using bestglm
bg.bic <- bestglm(df.train, family = gaussian, IC = "BIC")
knitr::opts_chunk$set(echo = TRUE)
df <- read.csv("stellar_temperature.csv")
summary(df)
nrow(df)
library(ggplot2)
ggplot(data = df , mapping = aes(y = df$teff)) +
geom_boxplot(outliers = TRUE , outlier.color = "darkred" , fill = "seagreen") +
labs(title = "Boxplot of Effective Temperature (teff)" ,
y = "Effective Temperature")
library(reshape2)
library(dplyr)
# Melt the dataframe to long format for ggplot
df_pred <- df %>% select(-teff)
df_long <- melt(df_pred , id.vars = NULL, measure.vars = names(df_pred))
# Plot multiple boxplots
ggplot(df_long, aes(x = variable, y = value)) +
geom_boxplot(fill = "black" , outlier.colour = "red", outlier.shape = 16, outlier.size = 2) +
labs(title = "Boxplots of All Variables", x = "Variables", y = "Values") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
theme_minimal()
outliers_boxplot <- boxplot.stats(df$teff)$out
outliers_boxplot
library(ggplot2)
missing_data <- sapply(df, function(x) sum(is.na(x)))
missing_data
sum(is.na(df))
df.new<-df
sapply(df.new, class)
library(ggplot2)
# Function to create a histogram for a specific variable
create_histogram <- function(df, var) {
ggplot(df, aes_string(x = var)) +  # aes_string() allows dynamic variable names
geom_histogram(fill = "brown", color = "white", bins = 30) +
ggtitle(paste("Histogram of", var)) +
theme_minimal()
}
# Get the names of all numeric variables in the dataset
vars <- names(df)
# Loop through all numeric variables and plot histograms
for (var in vars) {
print(create_histogram(df, var))  # Display the histogram
}
df$parallax= sqrt(df$parallax)
ggplot(df, aes_string(x = df$parallax)) +  # aes_string() allows dynamic variable names
geom_histogram(fill = "seagreen", color = "white", bins = 30) +
ggtitle(paste("Histogram of Parallax")) +
xlab("parallax") +
theme_minimal()
df$br_col = log(df$br_col)
ggplot(df, aes_string(x = df$br_col)) +  # aes_string() allows dynamic variable names
geom_histogram(fill = "seagreen", color = "white", bins = 30) +
ggtitle(paste("Histogram of Br_col")) +
xlab("Brcol")
theme_minimal()
df <- na.omit(df)
cor_matrix <- cor(df[, sapply(df, is.numeric)])
library(corrplot)
corrplot(cor_matrix, method = "circle")
set.seed(101)
s <- sample(length(df$teff) , round(0.7*length(df$teff)))
df.train <- df[s,]
df.test <- df[-s,]
lm.out <- lm(teff ~ ., data = df.train)
summary(lm.out)
y.hat = predict(lm.out, newdata = df.test)
# valid_idx <- !is.na(y.hat) & !is.na(df.test$teff)
# mse <- mean((df.test$teff[valid_idx] - y.hat[valid_idx])^2)
mse <- mean((df.test$teff - y.hat)^2)
residuals <- df.test$teff - y.hat
print(mse)
# Residual Plot
ggplot(data = data.frame(y.hat, residuals), aes(x = y.hat, y = residuals)) +
geom_point() +
geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
labs(title = "Residuals vs Predicted Values", x = "Predicted Values", y = "Residuals") +
theme_minimal()
# Histogram of residuals to check normality
ggplot(data = data.frame(residuals), aes(x = residuals)) +
geom_histogram(binwidth = 100, fill = "blue", color = "white") +
labs(title = "Histogram of Residuals", x = "Residuals", y = "Frequency") +
theme_minimal()
rmse <- sqrt(mse)
print(rmse)
df_plot <- data.frame(
Observed = df.test$teff,
Predicted = y.hat
)
axis_limits <- range(c(df_plot$Observed, df_plot$Predicted))
# Plot: Observed vs Predicted with a diagonal line
ggplot(df_plot, aes(x = Observed, y = Predicted)) +
geom_point(alpha = 0.6, color = "blue") +  # Points for each observation
geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed", linewidth = 1) +  # Diagonal line
xlim(axis_limits) + ylim(axis_limits) +  # Same limits for both axes
labs(title = "Observed vs Predicted Values", x = "Observed", y = "Predicted") +
theme_minimal()
suppressMessages(library(car))
reduced_df <- df.train
lm.out <- lm(teff~.,data=reduced_df)
vif(lm.out)
library(dplyr)
library(bestglm)
set.seed(101)
# Ensure 'y' is numeric after renaming
df.renamed <- df %>% rename(y = teff)
df.renamed$y <- as.numeric(df.renamed$y)  # Ensure y is numeric
# Split the data into training and test sets
s <- sample(length(df.renamed$y), round(0.7 * length(df.renamed$y)))
df.train <- df.renamed[s, ]
df.test <- df.renamed[-s, ]
df.train <- na.omit(df.train)
df.test <- na.omit(df.test)
# Fit the full linear model
lm.out <- lm(y ~ ., data = df.train)
# Compute the MSE for the full model
mse.full <- mean((predict(lm.out, newdata = df.test) - df.test$y) ^ 2)  # df.test$y, not df.test$teff
print(paste("MSE of the full model:", mse.full))
# Perform best subset selection using bestglm
bg.bic <- bestglm(df.train, family = gaussian, IC = "BIC")
bg.bic$BestModel
bg.aic <- bestglm(df.train, family = gaussian, IC = "AIC")
bg.aic$BestModel
# Number of predictors in BIC-selected model (excluding intercept)
bic_predictors <- length(coef(bg.bic$BestModel)) - 1
print(bic_predictors)
# Number of predictors in AIC-selected model (excluding intercept)
aic_predictors <- length(coef(bg.aic$BestModel)) - 1
print(aic_predictors)
#BSS MSE
bic_mse <- mean((df.test$y - bic_predictors)^2)
print(bic_mse)
# Replace NA values in each column with the column mean
df <- data.frame(lapply(df, function(x) {
if (is.numeric(x)) {
x[is.na(x)] <- mean(x, na.rm = TRUE)
}
return(x)
}))
pca.out <- prcomp(df[,-1] , scale = TRUE)
v <- pca.out$sdev^2
# round(cumsum(v/sum(v)) , 3)
pve <- v / sum(v)
ggplot(data.frame(p=1:ncol(df[,-1]),c = cumsum(pve)),mapping=aes(x=p,y=c)) +
geom_point()
round(pca.out$rotation[,1:10],9)
set.seed(120)
df.pc <- data.frame(pca.out$x,df$teff) %>% select(-PC10, -PC11)
names(df.pc) <- c("PC1","PC2","PC3","PC4","PC5","PC6","PC7","PC8","PC9","y")
summary(df.pc)
lm.out <- lm(y ~.,data=df.pc,subset=s)
lm.pred <- predict(lm.out,newdata=df.pc[-s,])
mean((lm.pred-df.pc$y[-s])^2)
knitr::opts_chunk$set(echo = TRUE)
df <- read.csv("stellar_temperature.csv")
summary(df)
nrow(df)
library(ggplot2)
ggplot(data = df , mapping = aes(y = df$teff)) +
geom_boxplot(outliers = TRUE , outlier.color = "darkred" , fill = "seagreen") +
labs(title = "Boxplot of Effective Temperature (teff)" ,
y = "Effective Temperature")
library(reshape2)
library(dplyr)
# Melt the dataframe to long format for ggplot
df_pred <- df %>% select(-teff)
df_long <- melt(df_pred , id.vars = NULL, measure.vars = names(df_pred))
# Plot multiple boxplots
ggplot(df_long, aes(x = variable, y = value)) +
geom_boxplot(fill = "black" , outlier.colour = "red", outlier.shape = 16, outlier.size = 2) +
labs(title = "Boxplots of All Variables", x = "Variables", y = "Values") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
theme_minimal()
outliers_boxplot <- boxplot.stats(df$teff)$out
outliers_boxplot
library(ggplot2)
missing_data <- sapply(df, function(x) sum(is.na(x)))
missing_data
sum(is.na(df))
df.new<-df
sapply(df.new, class)
library(ggplot2)
# Function to create a histogram for a specific variable
create_histogram <- function(df, var) {
ggplot(df, aes_string(x = var)) +  # aes_string() allows dynamic variable names
geom_histogram(fill = "brown", color = "white", bins = 30) +
ggtitle(paste("Histogram of", var)) +
theme_minimal()
}
# Get the names of all numeric variables in the dataset
vars <- names(df)
# Loop through all numeric variables and plot histograms
for (var in vars) {
print(create_histogram(df, var))  # Display the histogram
}
df$parallax= sqrt(df$parallax)
ggplot(df, aes_string(x = df$parallax)) +  # aes_string() allows dynamic variable names
geom_histogram(fill = "seagreen", color = "white", bins = 30) +
ggtitle(paste("Histogram of Parallax")) +
xlab("parallax") +
theme_minimal()
df$br_col = sqrt(df$br_col)
ggplot(df, aes_string(x = df$br_col)) +  # aes_string() allows dynamic variable names
geom_histogram(fill = "seagreen", color = "white", bins = 30) +
ggtitle(paste("Histogram of Br_col")) +
xlab("Brcol")
theme_minimal()
df <- na.omit(df)
cor_matrix <- cor(df[, sapply(df, is.numeric)])
library(corrplot)
corrplot(cor_matrix, method = "circle")
set.seed(101)
s <- sample(length(df$teff) , round(0.7*length(df$teff)))
df.train <- df[s,]
df.test <- df[-s,]
lm.out <- lm(teff ~ ., data = df.train)
summary(lm.out)
y.hat = predict(lm.out, newdata = df.test)
# valid_idx <- !is.na(y.hat) & !is.na(df.test$teff)
# mse <- mean((df.test$teff[valid_idx] - y.hat[valid_idx])^2)
mse <- mean((df.test$teff - y.hat)^2)
residuals <- df.test$teff - y.hat
print(mse)
# Residual Plot
ggplot(data = data.frame(y.hat, residuals), aes(x = y.hat, y = residuals)) +
geom_point() +
geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
labs(title = "Residuals vs Predicted Values", x = "Predicted Values", y = "Residuals") +
theme_minimal()
# Histogram of residuals to check normality
ggplot(data = data.frame(residuals), aes(x = residuals)) +
geom_histogram(binwidth = 100, fill = "blue", color = "white") +
labs(title = "Histogram of Residuals", x = "Residuals", y = "Frequency") +
theme_minimal()
rmse <- sqrt(mse)
print(rmse)
df_plot <- data.frame(
Observed = df.test$teff,
Predicted = y.hat
)
axis_limits <- range(c(df_plot$Observed, df_plot$Predicted))
# Plot: Observed vs Predicted with a diagonal line
ggplot(df_plot, aes(x = Observed, y = Predicted)) +
geom_point(alpha = 0.6, color = "blue") +  # Points for each observation
geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed", linewidth = 1) +  # Diagonal line
xlim(axis_limits) + ylim(axis_limits) +  # Same limits for both axes
labs(title = "Observed vs Predicted Values", x = "Observed", y = "Predicted") +
theme_minimal()
suppressMessages(library(car))
reduced_df <- df.train
lm.out <- lm(teff~.,data=reduced_df)
vif(lm.out)
library(dplyr)
library(bestglm)
set.seed(101)
# Ensure 'y' is numeric after renaming
df.renamed <- df %>% rename(y = teff)
df.renamed$y <- as.numeric(df.renamed$y)  # Ensure y is numeric
# Split the data into training and test sets
s <- sample(length(df.renamed$y), round(0.7 * length(df.renamed$y)))
df.train <- df.renamed[s, ]
df.test <- df.renamed[-s, ]
df.train <- na.omit(df.train)
df.test <- na.omit(df.test)
# Fit the full linear model
lm.out <- lm(y ~ ., data = df.train)
# Compute the MSE for the full model
mse.full <- mean((predict(lm.out, newdata = df.test) - df.test$y) ^ 2)  # df.test$y, not df.test$teff
print(paste("MSE of the full model:", mse.full))
# Perform best subset selection using bestglm
bg.bic <- bestglm(df.train, family = gaussian, IC = "BIC")
bg.bic$BestModel
bg.aic <- bestglm(df.train, family = gaussian, IC = "AIC")
bg.aic$BestModel
# Number of predictors in BIC-selected model (excluding intercept)
bic_predictors <- length(coef(bg.bic$BestModel)) - 1
print(bic_predictors)
# Number of predictors in AIC-selected model (excluding intercept)
aic_predictors <- length(coef(bg.aic$BestModel)) - 1
print(aic_predictors)
#BSS MSE
bic_mse <- mean((df.test$y - bic_predictors)^2)
print(bic_mse)
# Replace NA values in each column with the column mean
df <- data.frame(lapply(df, function(x) {
if (is.numeric(x)) {
x[is.na(x)] <- mean(x, na.rm = TRUE)
}
return(x)
}))
pca.out <- prcomp(df[,-1] , scale = TRUE)
v <- pca.out$sdev^2
# round(cumsum(v/sum(v)) , 3)
pve <- v / sum(v)
ggplot(data.frame(p=1:ncol(df[,-1]),c = cumsum(pve)),mapping=aes(x=p,y=c)) +
geom_point()
round(pca.out$rotation[,1:10],9)
set.seed(120)
df.pc <- data.frame(pca.out$x,df$teff) %>% select(-PC10, -PC11)
names(df.pc) <- c("PC1","PC2","PC3","PC4","PC5","PC6","PC7","PC8","PC9","y")
summary(df.pc)
lm.out <- lm(y ~.,data=df.pc,subset=s)
lm.pred <- predict(lm.out,newdata=df.pc[-s,])
mean((lm.pred-df.pc$y[-s])^2)
library(dplyr)
library(bestglm)
set.seed(101)
# Ensure 'y' is numeric after renaming
df.renamed <- df %>% rename(y = teff)
df.renamed$y <- as.numeric(df.renamed$y)  # Ensure y is numeric
# Split the data into training and test sets
s <- sample(length(df.renamed$y), round(0.7 * length(df.renamed$y)))
df.train <- df.renamed[s, ]
df.test <- df.renamed[-s, ]
df.train <- na.omit(df.train)
df.test <- na.omit(df.test)
# Fit the full linear model
lm.out <- lm(y ~ ., data = df.train)
# Compute the MSE for the full model
mse.full <- mean((predict(lm.out, newdata = df.test) - df.test$y) ^ 2)  # df.test$y, not df.test$teff
print(paste("MSE of the full model:", mse.full))
# Perform best subset selection using bestglm
bg.bic <- bestglm(df.train, family = gaussian, IC = "BIC")
bg.bic$BestModel
bg.aic <- bestglm(df.train, family = gaussian, IC = "AIC")
bg.aic$BestModel
# Number of predictors in BIC-selected model (excluding intercept)
bic_predictors <- length(coef(bg.bic$BestModel)) - 1
print(bic_predictors)
# Number of predictors in AIC-selected model (excluding intercept)
aic_predictors <- length(coef(bg.aic$BestModel)) - 1
print(aic_predictors)
#BSS MSE
bic_mse <- mean((df.test$y - bic_predictors)^2)
print(bic_mse)
set.seed(120)
df.pc <- data.frame(pca.out$x,df$teff) %>% select(-PC10, -PC11)
names(df.pc) <- c("PC1","PC2","PC3","PC4","PC5","PC6","PC7","PC8","PC9","y")
summary(df.pc)
lm.out <- lm(y ~.,data=df.pc,subset=s)
lm.pred <- predict(lm.out,newdata=df.pc[-s,])
mean((lm.pred-df.pc$y[-s])^2)
