---
title: "Lab: Basic String Manipulation"
author: "36-600"
output:
  html_document:
    toc: no
    toc_float: no
    theme: spacelab
  pdf_document:
    toc: no
---

## Data

Below we read in Joe Biden's 2021 inauguration speech as formatted by the White House on its website (with one exception: I concatenated the lines containing a poem together into one line).
```{r}
lines = readLines("http://www.stat.cmu.edu/~pfreeman/biden_2021.txt")
```

## Question 1

How many lines are there? How many characters are there, overall? (This includes spaces and punctuation, for now.)
```{r}
# FILL ME IN
length(lines)
sum(nchar(lines))
```

## Question 2

How many spaces are there in the speech, as formatted? (Don't worry about the fact that there would be spaces between the lines if they were all concatenated together.) One way to do this is to use `gregexpr()` to identify every place where there are spaces, then use a for-loop to loop over the output from that function and count the number of spaces. For instance:
```
out = [output from some function call]
total = 0
for ( ii in 1:length(lines) ) {
  total = total+length(out[[ii]])
}
```

```{r}
# FILL ME IN
out = gregexpr(" ", lines)
total = 0
for ( ii in 1:length(lines) ) {
  total = total+length(out[[ii]])
}
print(total)
```

## Question 3

Create a table showing how many words are on each line of the speech. For our purposes, words are separated by spaces. Utilize `strsplit()`. The output will be a list, where each element shows the individual words from a speech line. Determine the total number of words for each line, put the results in a vector, and run `table()` with that vector as input. You should find that nine of the lines have one word, etc. (Note that you'll utilize a for-loop again, in a manner similar to the last question.)
```{r}
# FILL ME IN
words <- strsplit(lines , " ")
count <- numeric(length(lines))

# Loop over the list of words per line and count the number of words
for (ii in 1:length(lines)) {
  count[ii] <- length(words[[ii]])
}

# Create a table of the number of words per line
word_count_table <- table(count)

# Display the table
word_count_table
```

## Question 4

Define a variable called `america` which is true if the word "America" is observed in a speech line, and false otherwise. Run `sum()` on that variable to see how many lines have "America" in it. Don't overthink this: you can do this in one line utilizing `grepl()`.
```{r}
# FILL ME IN
america <- sum(grepl("America" , lines))
america
```

## Question 5

Concatenate Biden's inaugural speech into a single line. Call the output `speech`. Make sure that you insert a space between the end of each of the old lines and the beginning of the next lines. (See our use of the `collapse` argument in `paste()`.)
```{r}
# FILL ME IN
speech <- paste(lines , collapse = " ")
speech
```

## Question 6

Working either with `lines` or with `speech`, utilize the framework on the last slide of the notes to remove punctuation and stopwords, leaving a single line speech in the end.
```{r}
# FILL ME IN
suppressMessages(library(stopwords))
head(stopwords("en"),10)

speech <- tolower(unlist(strsplit(lines,split="[ ,!\\.\\?;:\\-]")))
w <- which(nchar(speech)==0)
speech <- speech[-w] # could do speech <- speech[speech!=""] also, or dplyr...
stopword.logical <- speech %in% stopwords("en") # is element of left "in" vector at right? [T/F]
paste(speech[stopword.logical==FALSE],collapse=" ")
```

## Question 7

What are the top 20 words (meaning, non-stopwords) in Biden's speech? You might notice that "America" appears less than you'd expect, given your result above...but when you searched on "America" above, you probably also found "American" and "Americans," etc. (Unless you crafted a really exact regex!)
```{r}
# FILL ME IN
filtered_speech <- speech[!stopword.logical]

# Create a frequency table for the remaining words
word_frequency <- table(filtered_speech)

# Sort the words by their frequency in descending order
sorted_words <- sort(word_frequency, decreasing = TRUE)

# Get the top 20 most common words
top_20_words <- head(sorted_words, 20)

# Display the top 20 words
top_20_words
```

