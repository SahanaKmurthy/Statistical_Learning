# Remove rows with outliers
data_cleaned <- df_clean %>%
filter(if_any(starts_with("z_"), ~ abs(.) <= 3))
nrow(data_cleaned)
# Function to calculate Z-scores
calculate_z_scores <- function(x) {
(x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
}
# Add Z-scores for numeric predictors only
z_scores <- df_clean %>%
mutate(across(where(is.numeric), calculate_z_scores, .names = "z_{col}"))
summary(z_scores)
# Identify rows with outliers (|Z| > 3 in any column)
outliers <- z_scores %>%
filter(if_any(starts_with("z_"), ~ abs(.) > 3))
cat("Outliers:", nrow(outliers), "rows\n")
# Remove rows with outliers
# data_cleaned <- df_clean %>%
#  filter(if_any(starts_with("z_"), ~ abs(.) <= 3))
data_cleaned <- data_cleaned[apply(data_cleaned[, c("z_fix.acid", "z_vol.acid", "z_citric", "z_sugar", "z_chlorides", "z_free.sd", "z_total.sd", "z_density", "z_pH", "z_sulphates", "z_alcohol")], 1, function(x) all(abs(x) <= 3)), ]
suppressMessages(library(dplyr))
df_clean <- df %>% filter(complete.cases(.))
nrow(df)
# Function to calculate Z-scores
calculate_z_scores <- function(x) {
(x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
}
# Add Z-scores for numeric predictors only
z_scores <- df_clean %>%
mutate(across(where(is.numeric), calculate_z_scores, .names = "z_{col}"))
summary(z_scores)
# Identify rows with outliers (|Z| > 3 in any column)
outliers <- z_scores %>%
filter(if_any(starts_with("z_"), ~ abs(.) > 3))
cat("Outliers:", nrow(outliers), "rows\n")
# Remove rows with outliers
# data_cleaned <- df_clean %>%
#  filter(if_any(starts_with("z_"), ~ abs(.) <= 3))
data_cleaned <- data_cleaned[apply(data_cleaned[, c("z_fix.acid", "z_vol.acid", "z_citric", "z_sugar", "z_chlorides", "z_free.sd", "z_total.sd", "z_density", "z_pH", "z_sulphates", "z_alcohol")], 1, function(x) all(abs(x) <= 3)), ]
# Function to calculate Z-scores
calculate_z_scores <- function(x) {
(x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
}
# Add Z-scores for numeric predictors only
z_scores <- df_clean %>%
mutate(across(where(is.numeric), calculate_z_scores, .names = "z_{col}"))
summary(z_scores)
# Identify rows with outliers (|Z| > 3 in any column)
outliers <- z_scores %>%
filter(if_any(starts_with("z_"), ~ abs(.) > 3))
cat("Outliers:", nrow(outliers), "rows\n")
# Remove rows with outliers
data_cleaned <- df_clean %>% filter(if_any(starts_with("z_"), ~ abs(.) <= 3))
data_cleaned <- data_cleaned[apply(data_cleaned[, c("z_fix.acid", "z_vol.acid", "z_citric", "z_sugar", "z_chlorides", "z_free.sd", "z_total.sd", "z_density", "z_pH", "z_sulphates", "z_alcohol")], 1, function(x) all(abs(x) <= 3)), ]
# Function to calculate Z-scores
calculate_z_scores <- function(x) {
(x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
}
# Add Z-scores for numeric predictors only in df_clean
z_scores <- df_clean %>%
mutate(across(where(is.numeric), calculate_z_scores, .names = "z_{col}"))
summary(z_scores)
# Identify rows with outliers (|Z| > 3 in any column)
outliers <- z_scores %>%
filter(if_any(starts_with("z_"), ~ abs(.) > 3))
cat("Outliers:", nrow(outliers), "rows\n")
# Remove rows with outliers from df_clean
data_cleaned <- df_clean %>%
filter(if_all(starts_with("z_"), ~ abs(.) <= 3))
# Remove the 'z_*' columns before final output
data_cleaned <- data_cleaned %>%
select(-starts_with("z_"))
nrow(data_cleaned)  # Check the number of rows after outlier removal
# Function to calculate Z-scores
calculate_z_scores <- function(x) {
(x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
}
# Add Z-scores for numeric predictors only in df_clean
z_scores <- df_clean %>%
mutate(across(where(is.numeric), calculate_z_scores, .names = "z_{col}"))
summary(z_scores)
# Identify rows with outliers (|Z| > 3 in any column)
outliers <- z_scores %>%
filter(if_any(starts_with("z_"), ~ abs(.) > 3))
cat("Outliers:", nrow(outliers), "rows\n")
# Remove rows with outliers from df_clean
data_cleaned <- z_scores %>%
filter(if_all(!starts_with("z_") | abs(.) <= 3))
suppressMessages(library(dplyr))
df_clean <- df %>% filter(complete.cases(.))
nrow(df)
# Function to calculate Z-scores
calculate_z_scores <- function(x) {
(x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
}
# Add Z-scores for numeric predictors only in df_clean
z_scores <- df_clean %>%
mutate(across(where(is.numeric), calculate_z_scores, .names = "z_{col}"))
summary(z_scores)
# Identify rows with outliers (|Z| > 3 in any column)
outliers <- z_scores %>%
filter(if_any(starts_with("z_"), ~ abs(.) > 3))
cat("Outliers:", nrow(outliers), "rows\n")
# Remove rows with outliers from df_clean
data_cleaned <- z_scores %>%
filter(if_all(!starts_with("z_") | abs(.) <= 3))
# Function to calculate Z-scores
calculate_z_scores <- function(x) {
(x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
}
# Add Z-scores for numeric predictors only in df_clean
z_scores <- df_clean %>%
mutate(across(where(is.numeric), calculate_z_scores, .names = "z_{col}"))
summary(z_scores)
# Identify rows with outliers (|Z| > 3 in any column)
outliers <- z_scores %>%
filter(if_any(starts_with("z_"), ~ abs(.) > 3))
cat("Outliers:", nrow(outliers), "rows\n")
# Remove rows with outliers from df_clean
data_cleaned <- z_scores %>%
filter(if_all(where(is.numeric), ~ !starts_with("z_", .) | abs(.) <= 3))
# Function to calculate Z-scores
calculate_z_scores <- function(x) {
(x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
}
# Add Z-scores for numeric predictors only in df_clean
z_scores <- df_clean %>%
mutate(across(where(is.numeric), calculate_z_scores, .names = "z_{col}"))
summary(z_scores)
# Identify rows with outliers (|Z| > 3 in any column)
outliers <- z_scores %>%
filter(if_any(starts_with("z_"), ~ abs(.) > 3))
cat("Outliers:", nrow(outliers), "rows\n")
# Remove rows with outliers from df_clean
data_cleaned <- z_scores %>%
filter(if_all(where(is.numeric), ~ !str_detect(cur_column(), "^z_") | abs(.) <= 3))
# Function to calculate Z-scores
calculate_z_scores <- function(x) {
(x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
}
# Add Z-scores for numeric predictors only in df_clean
z_scores <- df_clean %>%
mutate(across(where(is.numeric), calculate_z_scores, .names = "z_{col}"))
summary(z_scores)
# Identify rows with outliers (|Z| > 3 in any column)
outliers <- z_scores %>%
filter(if_any(starts_with("z_"), ~ abs(.) > 3))
cat("Outliers:", nrow(outliers), "rows\n")
# Remove rows with outliers from df_clean
data_cleaned <- z_scores %>%
filter(across(where(is.numeric), ~ !str_detect(cur_column(), "^z_") | abs(.) <= 3))
# Remove the 'z_*' columns before final output
data_cleaned <- data_cleaned %>%
select(-starts_with("z_"))
nrow(data_cleaned)  # Check the number of rows after outlier removal
df <- read.csv("wineQuality.csv" , stringsAsFactors = TRUE)
summary(df)
nrow(df)
# Function to calculate Z-scores
calculate_z_scores <- function(x) {
(x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
}
# Add Z-scores for numeric predictors only in df_clean
z_scores <- df_clean %>%
mutate(across(where(is.numeric), calculate_z_scores, .names = "z_{col}"))
summary(z_scores)
# Identify rows with outliers (|Z| > 3 in any column)
outliers <- z_scores %>%
filter(if_any(starts_with("z_"), ~ abs(.) > 3))
cat("Outliers:", nrow(outliers), "rows\n")
# Remove rows with outliers from df_clean
data_cleaned <- z_scores %>%
filter(across(where(is.numeric), ~ !str_detect(cur_column(), "^z_") | abs(.) <= 3))
# Remove the 'z_*' columns before final output
data_cleaned <- data_cleaned %>%
select(-starts_with("z_"))
nrow(data_cleaned)  # Check the number of rows after outlier removal
set.seed(100)
s <- sample(length(data_cleaned$label) , round(0.7*length(data_cleaned$label)))
df.train <- data_cleaned[s,]
df.test <- data_cleaned[-s,]
log.out <- glm(label ~ . , data = df.train , family = binomial)
resp.prob = predict(log.out,newdata=df.test,type="response")
suppressMessages(library(pROC))
(roc.log <- roc(df.test$label,resp.prob))
plot(roc.log,col="red",xlim=c(1,0),ylim=c(0,1))
roc.log
log.auc <- round(roc.log$auc,3)
cat("AUC for logistic regression: ",log.auc,"\n")
log.J <- roc.log$sensitivities + roc.log$specificities - 1
w <- which.max(log.J)
cat("Optimum threshold for logistic regression: ",round(roc.log$thresholds[w],3),"\n")
optimal_threshold <- roc.log$thresholds[w]
resp.pred = ifelse(resp.prob>optimal_threshold,"GOOD","BAD")
log.mcr <- round(mean(resp.pred!= df.test$label),3) # compressed MCR calculator
tab <- table(Predicted = resp.pred, Actual = df.test$label)
cat("Misclassification rate for logistic regression: " ,log.mcr)
print(tab)
suppressMessages(library(tidyverse))
suppressMessages(library(randomForest))
rf.out = randomForest(label~.,data=df.train,
importance=TRUE)
# Get predicted probabilities for the "GOOD" class
resp.prob <- predict(rf.out, newdata = df.test, type = "prob")[, "GOOD"]
(roc.rf <- roc(df.test$label,resp.prob))
plot(roc.rf,col="red",xlim=c(1,0),ylim=c(0,1))
roc.rf
rf.auc <- round(roc.rf$auc,3)
cat("AUC for Random Forest: ",round(roc.rf$auc,3),"\n")
rf.J <- roc.rf$sensitivities + roc.rf$specificities - 1
w <- which.max(rf.J)
rf.J.tab <- rf.J[w]
optimal_threshold <- roc.rf$thresholds[w]
cat("Optimum threshold for Random Forest: ",optimal_threshold,"\n")
# Use the optimal threshold to classify the test data
resp.pred <- ifelse(resp.prob > optimal_threshold, "GOOD", "BAD")
# Calculate MCR
rf.mcr <- round(mean(resp.pred != df.test$label), 3)
# Print MCR
print(paste("Random Forest MCR:", rf.mcr))
confusion_matrix <- table(Predicted = resp.pred, Actual = df.test$label)
confusion_df <- as.data.frame(confusion_matrix)
ggplot(confusion_df, aes(x = Actual, y = Predicted, fill = Freq)) +
geom_tile(color = "black") +
geom_text(aes(label = Freq), color = "white", size = 5) +
scale_fill_gradient(low = "lightblue", high = "blue") +
labs(title = "Confusion Matrix", x = "Actual Labels", y = "Predicted Labels") +
theme_minimal()
set.seed(101)
suppressMessages(library(xgboost))
# Convert "GOOD" and "BAD" to numeric labels (1 for "GOOD", 0 for "BAD")
df.train$label <- as.numeric(df.train$label == "GOOD")
df.test$label <- as.numeric(df.test$label == "GOOD")
train <- xgb.DMatrix(data=as.matrix(df.train[,1:10]),label=df.train$label)
test <- xgb.DMatrix(data=as.matrix(df.test[,1:10]),label=df.test$label)
# Hyperparameters
params <- list(
objective = "binary:logistic", # For binary classification
eta = 0.1,                     # Learning rate
max_depth = 6,                 # Maximum tree depth
min_child_weight = 1,          # Minimum sum of instance weight in a child
subsample = 0.8,               # Row sampling ratio
colsample_bytree = 0.8         # Feature sampling ratio
)
# Cross-validation
xgb.cv.out <- xgb.cv(
params = params,
data = train,
nrounds = 30,
nfold = 5,
verbose = 0,
metrics = "auc" # Evaluate using AUC
)
# Identify the optimal number of trees
optimal_trees <- which.max(xgb.cv.out$evaluation_log$test_auc_mean)
auc.max <- max(xgb.cv.out$evaluation_log$test_auc_mean)
cat("The optimal number of trees is ", which.max(xgb.cv.out$evaluation_log$test_auc_mean), "\n")
cat("Best AUC: ", round(auc.max, 3), "\n")
# Train final model
xgb.out <- xgboost(
data = train,
nrounds = optimal_trees,
params = params,
verbose = 0
)
# Predict probabilities
resp.prob <- predict(xgb.out, newdata = test)
# Get predicted probabilities for the "GOOD" class
(roc.gbm <- roc(df.test$label,resp.prob))
plot(roc.gbm,col="red",xlim=c(1,0),ylim=c(0,1))
roc.gbm
xgb.auc <- round(roc.gbm$auc,3)
cat("AUC for gradient boosting: ",xgb.auc,"\n")
xgb.J <- roc.gbm$sensitivities + roc.gbm$specificities - 1
w <- which.max(xgb.J)
cat("Optimum threshold for Gradient Boosting: ",round(roc.log$thresholds[w],3),"\n")
# Convert probabilities to predictions using the optimal threshold
resp.pred <- ifelse(resp.prob > optimal_threshold, 1, 0)
# Calculate Misclassification Rate (MCR)
xgb.mcr <- mean(resp.pred != df.test$label) # Now both are numeric
cat("Misclassification Rate (MCR): ", round(xgb.mcr, 3), "\n")
# Evaluate performance
confusion.mat <- table(Predicted = resp.pred, Actual = ifelse(df.test$label == 1, "GOOD", "BAD"))
accuracy <- sum(diag(confusion.mat)) / sum(confusion.mat)
cat("Confusion Matrix:\n")
print(confusion.mat)
cat("Accuracy: ", round(accuracy, 3), "\n")
final_table <- data.frame(Model_Name = c("Logistic Regression" , "Random Forest" , "Gradient Boosting") , MCR = c(log.mcr , rf.mcr , xgb.mcr) , AUC = c(log.auc , rf.auc , xgb.auc) , J_statistic = c(log.J[w] , rf.J.tab , xgb.J[w]))
print(final_table)
plot(roc.log, col = "red", lwd = 2, main = "ROC Curves for Models")
plot(roc.rf, col = "blue", add = TRUE, lwd = 2)
plot(roc.gbm, col = "seagreen", add = TRUE, lwd = 2)
legend("bottomright", legend = c("Logistic", "Random Forest", "Gradient Boosting"),
col = c("red", "blue", "seagreen"), lwd = 2)
# Get predicted probabilities for the "GOOD" class
resp.prob <- predict(rf.out, newdata = df.test, type = "prob")[, "GOOD"]
(roc.rf <- roc(df.test$label,resp.prob))
plot(roc.rf,col="red",xlim=c(1,0),ylim=c(0,1))
roc.rf
rf.auc <- round(roc.rf$auc,3)
cat("AUC for Random Forest: ",round(roc.rf$auc,3),"\n")
rf.J <- roc.rf$sensitivities + roc.rf$specificities - 1
w <- which.max(rf.J)
rf.J.tab <- rf.J[w]
print(rf.J.tab)
optimal_threshold <- roc.rf$thresholds[w]
cat("Optimum threshold for Random Forest: ",optimal_threshold,"\n")
# Use the optimal threshold to classify the test data
resp.pred <- ifelse(resp.prob > optimal_threshold, "GOOD", "BAD")
# Calculate MCR
rf.mcr <- round(mean(resp.pred != df.test$label), 3)
# Print MCR
print(paste("Random Forest MCR:", rf.mcr))
confusion_matrix <- table(Predicted = resp.pred, Actual = df.test$label)
confusion_df <- as.data.frame(confusion_matrix)
ggplot(confusion_df, aes(x = Actual, y = Predicted, fill = Freq)) +
geom_tile(color = "black") +
geom_text(aes(label = Freq), color = "white", size = 5) +
scale_fill_gradient(low = "lightblue", high = "blue") +
labs(title = "Confusion Matrix", x = "Actual Labels", y = "Predicted Labels") +
theme_minimal()
rm(list = ls())
df <- read.csv("wineQuality.csv" , stringsAsFactors = TRUE)
summary(df)
nrow(df)
suppressMessages(library(dplyr))
df_clean <- df %>% filter(complete.cases(.))
nrow(df)
# Function to calculate Z-scores
calculate_z_scores <- function(x) {
(x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
}
# Add Z-scores for numeric predictors only in df_clean
z_scores <- df_clean %>%
mutate(across(where(is.numeric), calculate_z_scores, .names = "z_{col}"))
summary(z_scores)
# Identify rows with outliers (|Z| > 3 in any column)
outliers <- z_scores %>%
filter(if_any(starts_with("z_"), ~ abs(.) > 3))
cat("Outliers:", nrow(outliers), "rows\n")
# Remove rows with outliers from df_clean
data_cleaned <- z_scores %>%
filter(across(where(is.numeric), ~ !str_detect(cur_column(), "^z_") | abs(.) <= 3))
# Remove the 'z_*' columns before final output
data_cleaned <- data_cleaned %>%
select(-starts_with("z_"))
nrow(data_cleaned)  # Check the number of rows after outlier removal
suppressMessages(library(tidyverse))
# Function to calculate Z-scores
calculate_z_scores <- function(x) {
(x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
}
# Add Z-scores for numeric predictors only in df_clean
z_scores <- df_clean %>%
mutate(across(where(is.numeric), calculate_z_scores, .names = "z_{col}"))
summary(z_scores)
# Identify rows with outliers (|Z| > 3 in any column)
outliers <- z_scores %>%
filter(if_any(starts_with("z_"), ~ abs(.) > 3))
cat("Outliers:", nrow(outliers), "rows\n")
# Remove rows with outliers from df_clean
data_cleaned <- z_scores %>%
filter(across(where(is.numeric), ~ !str_detect(cur_column(), "^z_") | abs(.) <= 3))
# Remove the 'z_*' columns before final output
data_cleaned <- data_cleaned %>%
select(-starts_with("z_"))
nrow(data_cleaned)  # Check the number of rows after outlier removal
rm(list = ls())
df <- read.csv("wineQuality.csv" , stringsAsFactors = TRUE)
summary(df)
nrow(df)
suppressMessages(library(dplyr))
df_clean <- df %>% filter(complete.cases(.))
nrow(df)
suppressMessages(library(tidyverse))
# Function to calculate Z-scores
calculate_z_scores <- function(x) {
(x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
}
# Add Z-scores for numeric predictors only in df_clean
z_scores <- df_clean %>%
mutate(across(where(is.numeric), calculate_z_scores, .names = "z_{col}"))
summary(z_scores)
# Identify rows with outliers (|Z| > 3 in any column)
outliers <- z_scores %>%
filter(if_any(starts_with("z_"), ~ abs(.) > 3))
cat("Outliers:", nrow(outliers), "rows\n")
# Remove rows with outliers from df_clean
data_cleaned <- z_scores %>%
filter(across(where(is.numeric), ~ !str_detect(cur_column(), "^z_") | abs(.) <= 3))
# Remove the 'z_*' columns before final output
data_cleaned <- data_cleaned %>%
select(-starts_with("z_"))
nrow(data_cleaned)  # Check the number of rows after outlier removal
set.seed(100)
s <- sample(length(data_cleaned$label) , round(0.7*length(data_cleaned$label)))
df.train <- data_cleaned[s,]
df.test <- data_cleaned[-s,]
log.out <- glm(label ~ . , data = df.train , family = binomial)
resp.prob = predict(log.out,newdata=df.test,type="response")
suppressMessages(library(pROC))
(roc.log <- roc(df.test$label,resp.prob))
plot(roc.log,col="red",xlim=c(1,0),ylim=c(0,1))
roc.log
log.auc <- round(roc.log$auc,3)
cat("AUC for logistic regression: ",log.auc,"\n")
log.J <- roc.log$sensitivities + roc.log$specificities - 1
w <- which.max(log.J)
cat("Optimum threshold for logistic regression: ",round(roc.log$thresholds[w],3),"\n")
optimal_threshold <- roc.log$thresholds[w]
resp.pred = ifelse(resp.prob>optimal_threshold,"GOOD","BAD")
log.mcr <- round(mean(resp.pred!= df.test$label),3) # compressed MCR calculator
tab <- table(Predicted = resp.pred, Actual = df.test$label)
cat("Misclassification rate for logistic regression: " ,log.mcr)
print(tab)
suppressMessages(library(tidyverse))
suppressMessages(library(randomForest))
rf.out = randomForest(label~.,data=df.train,
importance=TRUE)
# Get predicted probabilities for the "GOOD" class
resp.prob <- predict(rf.out, newdata = df.test, type = "prob")[, "GOOD"]
(roc.rf <- roc(df.test$label,resp.prob))
plot(roc.rf,col="red",xlim=c(1,0),ylim=c(0,1))
roc.rf
rf.auc <- round(roc.rf$auc,3)
cat("AUC for Random Forest: ",round(roc.rf$auc,3),"\n")
rf.J <- roc.rf$sensitivities + roc.rf$specificities - 1
w <- which.max(rf.J)
rf.J.tab <- rf.J[w]
print(rf.J.tab)
optimal_threshold <- roc.rf$thresholds[w]
cat("Optimum threshold for Random Forest: ",optimal_threshold,"\n")
# Use the optimal threshold to classify the test data
resp.pred <- ifelse(resp.prob > optimal_threshold, "GOOD", "BAD")
# Calculate MCR
rf.mcr <- round(mean(resp.pred != df.test$label), 3)
# Print MCR
print(paste("Random Forest MCR:", rf.mcr))
confusion_matrix <- table(Predicted = resp.pred, Actual = df.test$label)
confusion_df <- as.data.frame(confusion_matrix)
ggplot(confusion_df, aes(x = Actual, y = Predicted, fill = Freq)) +
geom_tile(color = "black") +
geom_text(aes(label = Freq), color = "white", size = 5) +
scale_fill_gradient(low = "lightblue", high = "blue") +
labs(title = "Confusion Matrix", x = "Actual Labels", y = "Predicted Labels") +
theme_minimal()
set.seed(101)
suppressMessages(library(xgboost))
# Convert "GOOD" and "BAD" to numeric labels (1 for "GOOD", 0 for "BAD")
df.train$label <- as.numeric(df.train$label == "GOOD")
df.test$label <- as.numeric(df.test$label == "GOOD")
train <- xgb.DMatrix(data=as.matrix(df.train[,1:10]),label=df.train$label)
test <- xgb.DMatrix(data=as.matrix(df.test[,1:10]),label=df.test$label)
# Hyperparameters
params <- list(
objective = "binary:logistic", # For binary classification
eta = 0.1,                     # Learning rate
max_depth = 6,                 # Maximum tree depth
min_child_weight = 1,          # Minimum sum of instance weight in a child
subsample = 0.8,               # Row sampling ratio
colsample_bytree = 0.8         # Feature sampling ratio
)
# Cross-validation
xgb.cv.out <- xgb.cv(
params = params,
data = train,
nrounds = 30,
nfold = 5,
verbose = 0,
metrics = "auc" # Evaluate using AUC
)
# Identify the optimal number of trees
optimal_trees <- which.max(xgb.cv.out$evaluation_log$test_auc_mean)
auc.max <- max(xgb.cv.out$evaluation_log$test_auc_mean)
cat("The optimal number of trees is ", which.max(xgb.cv.out$evaluation_log$test_auc_mean), "\n")
cat("Best AUC: ", round(auc.max, 3), "\n")
# Train final model
xgb.out <- xgboost(
data = train,
nrounds = optimal_trees,
params = params,
verbose = 0
)
# Predict probabilities
resp.prob <- predict(xgb.out, newdata = test)
# Get predicted probabilities for the "GOOD" class
(roc.gbm <- roc(df.test$label,resp.prob))
plot(roc.gbm,col="red",xlim=c(1,0),ylim=c(0,1))
roc.gbm
xgb.auc <- round(roc.gbm$auc,3)
cat("AUC for gradient boosting: ",xgb.auc,"\n")
xgb.J <- roc.gbm$sensitivities + roc.gbm$specificities - 1
w <- which.max(xgb.J)
cat("Optimum threshold for Gradient Boosting: ",round(roc.log$thresholds[w],3),"\n")
# Convert probabilities to predictions using the optimal threshold
resp.pred <- ifelse(resp.prob > optimal_threshold, 1, 0)
# Calculate Misclassification Rate (MCR)
xgb.mcr <- mean(resp.pred != df.test$label) # Now both are numeric
cat("Misclassification Rate (MCR): ", round(xgb.mcr, 3), "\n")
# Evaluate performance
confusion.mat <- table(Predicted = resp.pred, Actual = ifelse(df.test$label == 1, "GOOD", "BAD"))
accuracy <- sum(diag(confusion.mat)) / sum(confusion.mat)
cat("Confusion Matrix:\n")
print(confusion.mat)
cat("Accuracy: ", round(accuracy, 3), "\n")
final_table <- data.frame(Model_Name = c("Logistic Regression" , "Random Forest" , "Gradient Boosting") , MCR = c(log.mcr , rf.mcr , xgb.mcr) , AUC = c(log.auc , rf.auc , xgb.auc) , J_statistic = c(log.J[w] , rf.J.tab , xgb.J[w]))
print(final_table)
plot(roc.log, col = "red", lwd = 2, main = "ROC Curves for Models")
plot(roc.rf, col = "blue", add = TRUE, lwd = 2)
plot(roc.gbm, col = "seagreen", add = TRUE, lwd = 2)
legend("bottomright", legend = c("Logistic", "Random Forest", "Gradient Boosting"),
col = c("red", "blue", "seagreen"), lwd = 2)
