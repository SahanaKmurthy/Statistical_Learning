---
title: "Lab: Numerical Optimization"
author: "36-600"
output: 
  html_document:
    toc: no
    toc_float: no
    theme: spacelab
---

## Data

Let's generate the same data that we used to illustrate nonlinear regression:
```{r}
set.seed(555)
x <- -5:5
y <- 0.1*x^3 - 0.5*x + 2.1 + rnorm(length(x),mean=0,sd=0.5*(1+abs(x)))
e <- 0.5*(1+abs(x))

df <- data.frame("x"=x,"y"=y,"e"=e)

suppressMessages(library(tidyverse))
ggplot(data=df,mapping=aes(x=x,y=y)) +
  geom_errorbar(aes(ymin=y-e, ymax=y+e), width=.1,color="blue") +
  geom_point(color="firebrick")
```

# Questions

## Question 1

Assume that you know that a cubic polynomial is the correct function to use here, but you do not know what the true parameters are. (That means you should include the $x^2$ term in your curve-fitting analysis! Yes...you could just do polynomial regression here, like you did in the last lab, but we'll go ahead and try out numerical optimization.) Code an optimizer that will allow you to estimate the four coefficient terms of a cubic polynomial. Try not to let the true cubic polynomial coefficients above influence your initial guesses. Show the coefficients and plot your result. Make sure your plot looks good before you move on to Question 2: with four terms, it can be relatively easy to find a locally optimal result that is not the globally optimal result, i.e., it can be relatively easy to find a local minimum in $\chi^2$ that is not the global minimum. It's pretty easy to identify when this is the case when you plot functions...so always plot them! 
```{r}
# REPLACE ME WITH CODE
# Define the objective function to calculate chi-squared for a cubic polynomial
fit.fun <- function(par, data) {
  # Extract data points and error values
  x <- data$x
  y <- data$y
  e <- data$e
  
  # Define the cubic polynomial model: a*x^3 + b*x^2 + c*x + d
  model <- par[1]*x^3 + par[2]*x^2 + par[3]*x + par[4]
  
  # Calculate the chi-square value
  chi_square <- sum((y - model)^2 / e^2)
  
  return(chi_square)
}

# Initial parameter guesses for a cubic polynomial (a, b, c, d)
initial_guesses <- c(1, 1, 1, 1) 

# Perform optimization to estimate the parameters
op.out <- optim(par = initial_guesses, fn = fit.fun, data = df, method = "BFGS")

# Extract the optimized coefficients
optimized_params <- op.out$par
print(optimized_params)  # Display the optimized parameters

# Add the predicted values to the dataframe for plotting
df <- df %>%
  mutate(predicted_y = optimized_params[1]*x^3 + 
                       optimized_params[2]*x^2 + 
                       optimized_params[3]*x + 
                       optimized_params[4])

# Plot the data points, error bars, and the fitted cubic polynomial
library(ggplot2)

ggplot(data = df, aes(x = x, y = y)) +
  geom_errorbar(aes(ymin = y - e, ymax = y + e), width = 0.1, color = "blue") +
  geom_point(color = "firebrick") +
  geom_line(aes(y = predicted_y), color = "darkgreen", size = 1) +
  labs(title = "Fitted Cubic Polynomial Model", y = "y", x = "x")
```

## Question 2

Take the minimum $\chi^2$ value that you found in Question 1 (it's in the output from `optim()`, even if you didn't explicitly display it) and perform a $\chi^2$ goodness of fit test. Recall that the null hypothesis is that the model is an acceptable one, i.e., that it plausibly replicates the data-generating process. Do you reject the null, or fail to reject the null? (Also recall that the number of degrees of freedom is $n-p$, where $n$ is the length of $x$ and $p$ is the number of coefficients in the cubic polynomial.)
```{r}
# REPLACE ME WITH CODE
min_chi_square = op.out$value
# number of data points
n <- length(df$x)

# number of parameters in cubic polynomila
p <- 4

#degrees of freedom
df <- n - p

#alpha
alpha <- 0.05

#Critical value from chi-square distribution 
critical_value <- qchisq(1 - alpha , df)

# Perform the chi-square goodness of fit test
if (min_chi_square > critical_value) {
  result <- "Reject the null hypothesis: The model does not fit the data well."
} else {
  result <- "Fail to reject the null hypothesis: The model fits the data well."
}

# Display the result
cat("Chi-square value:", min_chi_square, "\n")
cat("Critical value:", critical_value, "\n")
cat(result, "\n")
```
```
Replace me with text
```

## Data Part II

Now let's say we have a dataset that looks like this:
```{r}
df <- read.csv("https://www.stat.cmu.edu/~pfreeman/optim_data.csv")

suppressMessages(library(tidyverse))
ggplot(data=df,mapping=aes(x=x,y=y)) +
  geom_errorbar(aes(ymin=y-e, ymax=y+e), width=.1,color="blue") +
  geom_point(color="firebrick")
```

## Question 3

This will be a bit more free-form. Determine a model that might generate the observed data, optimize its parameters, and plot the result. Not sure where to start? Well, that's a common feeling in real-life analyses situations...
```{r}
# REPLACE ME WITH CODE
suppressMessages(library(tidyverse))

fit.fun <- function(par, data) {
  x <- data$x
  y <- data$y
  e <- data$e
  
  # Evaluate the model
  model <- par[1] * sin(par[2] * x + par[3]) + par[4] * x^2 + par[5] * x + par[6]
  
  # Calculate chi-square
  chi_square <- sum((y - model)^2 / e^2)
  
  return(chi_square)
}

# Set initial parameter guesses
par <- c(1, 0, 1, 1, 1, 0)  

# Perform optimization
op.out <- suppressWarnings(optim(par, fit.fun, data=df, method="BFGS"))

# Extract optimized parameters
optimized_params <- op.out$par

# Print the optimized parameters and chi-square value
print(optimized_params)
print(op.out$value)

# Plot the optimized model fit with the data
df <- df %>%
  mutate(predicted_y = optimized_params[1] * sin(optimized_params[2] * x + optimized_params[3]) + 
                       optimized_params[4] * x^2 + 
                       optimized_params[5] * x + 
                       optimized_params[6])

ggplot(df, aes(x=x)) +
  geom_errorbar(aes(ymin=y-e, ymax=y+e), width=.1, color="blue") +
  geom_point(aes(y=y), color="firebrick") +
  geom_line(aes(y=predicted_y), color="darkgreen", size=1) +
  labs(title="Fitted Sinusoidal + Polynomial Model", y="y", x="x")
```

